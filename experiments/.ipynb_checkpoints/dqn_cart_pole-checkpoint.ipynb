{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03edf560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# configure module & syspath\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.agents import DQN\n",
    "from src.policies import DQNCartPolePolicy\n",
    "from src.replays import VanillaReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "121d1112",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_pole_env = gym.make('CartPole-v1')\n",
    "policy = DQNCartPolePolicy(cart_pole_env.observation_space.shape[0], cart_pole_env.action_space.n)\n",
    "replay_memory = VanillaReplay(capacity = 500)\n",
    "optimizer = Adam(policy.parameters(), lr = 0.001)\n",
    "\n",
    "dqn = DQN(\n",
    "  env = cart_pole_env,\n",
    "  policy = policy,\n",
    "  replay_memory = replay_memory,\n",
    "  replay_size = 32,\n",
    "  optimizer = optimizer,\n",
    "  discount_rate = 0.999,\n",
    "  max_epsilon = 1.,\n",
    "  min_epsilon = 0.1,\n",
    "  epsilon_decay = 1e-3,\n",
    "  target_update_steps = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e16f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████████████▍                                                                                                                                                                                                                   | 34/500 [00:04<02:01,  3.84it/s]"
     ]
    }
   ],
   "source": [
    "max_episodes = 500\n",
    "rewards_last_10 = deque()\n",
    "\n",
    "plt_epsilon = list()\n",
    "plt_rewards_median = list()\n",
    "\n",
    "for epi in tqdm(range(max_episodes)):\n",
    "    episode_transitions = dqn.play_episode(tune = True)\n",
    "    rewards_last_10.append(np.sum(list(zip(*episode_transitions))[2]))\n",
    "\n",
    "    median_reward = np.median(rewards_last_10)\n",
    "    plt_rewards_median.append(median_reward)\n",
    "\n",
    "    if len(rewards_last_10) == 10:\n",
    "        rewards_last_10.popleft()\n",
    "    \n",
    "    # check if the policy is fully trained.\n",
    "    if median_reward > 190.:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(plt_rewards_m, label = 'Median Reward / 10 Episodes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
